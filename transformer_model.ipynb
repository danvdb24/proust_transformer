{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danvdb24/proust_transformer/blob/main/transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Np_0Ien0LA9"
      },
      "source": [
        "# Transformer model architecture\n",
        "![](https://github.com/PytLab/transformer-from-scratch/blob/master/images/transformer_architecture.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWI2iuaF0LBC"
      },
      "source": [
        "# Transformer implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "paYSBDp_0LBD"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kCJ_eqE0LBE"
      },
      "source": [
        "## Embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCcFAeKU0LBE"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xAqfXpr30LBF"
      },
      "outputs": [],
      "source": [
        "class Embedder(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super().__init__()\n",
        "        self.embed = torch.nn.Embedding(vocab_size, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # [123, 0, 23, 5] -> [[..512..], [...512...], ...]\n",
        "        return self.embed(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pti1CBHX0LBF"
      },
      "source": [
        "### Positional encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHd1x1AU0LBG"
      },
      "source": [
        "$$ PE_{(pos, 2i)} = sin(pos/10000^{2i/d_{model}}) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY6aClUq0LBG"
      },
      "source": [
        "$$ PE_{(pos, 2i + 1)} = cos(pos/10000^{2i/d_{model}}) $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qIVF_HUd0LBH"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoder(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, max_seq_len=80):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # create constant positional encoding matrix\n",
        "        pe_matrix = torch.zeros(max_seq_len, d_model)\n",
        "        \n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, d_model, 2):\n",
        "                pe_matrix[pos, i] = math.sin(pos/10000**(2*i/d_model))\n",
        "                pe_matrix[pos, i+1] = math.cos(pos/10000**(2*i/d_model))\n",
        "        pe_matrix = pe_matrix.unsqueeze(0)     # Add one dimension for batch size\n",
        "        self.register_buffer('pe', pe_matrix)  # Register as persistent buffer\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x is a sentence after embedding with dim (batch, number of words, vector dimension)\n",
        "        seq_len = x.size()[1]\n",
        "        x = x + self.pe[:, :seq_len]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejspsGGR0LBH"
      },
      "source": [
        "## Model layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW3oTDHd0LBH"
      },
      "source": [
        "### Scaled Dot-Product Attention layer\n",
        "\n",
        "![](https://github.com/PytLab/transformer-from-scratch/blob/master/images/scaled_dot_product_attention.png?raw=1)\n",
        "\n",
        "12*512\n",
        "\n",
        "[512..] x M = k_0\n",
        "[...512] x M = k_1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GcXGWeFI0LBH"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Given Query, Key, Value, calculate the final weighted value\n",
        "def scaled_dot_product_attention(q, k, v, mask=None, dropout=None):\n",
        "    # Shape of q and k are the same, both are (batch_size, seq_len, d_k)\n",
        "    # Shape of v is (batch_size, seq_len, d_v)\n",
        "    attention_scores = torch.matmul(q, k.transpose(-2, -1))/math.sqrt(q.shape[-1])  # size (batch_size, seq_len, seq_len)\n",
        "    \n",
        "    # Apply mask to scores\n",
        "    # <pad>\n",
        "    if mask is not None:\n",
        "        attention_scores = attention_scores.masked_fill(mask == 0, value=-1e9)\n",
        "        \n",
        "    # Softmax along the last dimension\n",
        "    attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "    \n",
        "    if dropout is not None:\n",
        "        attention_weights = dropout(attention_weights)\n",
        "        \n",
        "    output = torch.matmul(attention_weights, v)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TndN67Dn0LBI"
      },
      "source": [
        "### Multi-Head Attention layer\n",
        "\n",
        "![](https://github.com/PytLab/transformer-from-scratch/blob/master/images/multi_head_attention.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-OU1lPJ10LBI"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = self.d_v = d_model//n_heads\n",
        "        \n",
        "        # self attention linear layers\n",
        "        # Linear layers for q, k, v vectors generation in different heads\n",
        "        self.q_linear_layers = []\n",
        "        self.k_linear_layers = []\n",
        "        self.v_linear_layers = []\n",
        "        for i in range(n_heads):\n",
        "            self.q_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
        "            self.k_linear_layers.append(torch.nn.Linear(d_model, self.d_k))\n",
        "            self.v_linear_layers.append(torch.nn.Linear(d_model, self.d_v))\n",
        "        \n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.out = torch.nn.Linear(n_heads*self.d_v, d_model)\n",
        "        \n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        multi_head_attention_outputs = []\n",
        "        for q_linear, k_linear, v_linear in zip(self.q_linear_layers,\n",
        "                                                self.k_linear_layers,\n",
        "                                                self.v_linear_layers):\n",
        "            new_q = q_linear(q)  # size: (batch_size, seq_len, d_k)\n",
        "            new_k = k_linear(k)  # size: (batch_size, seq_len, d_k)\n",
        "            new_v = v_linear(v)  # size: (batch_size, seq_len, d_v)\n",
        "            \n",
        "            # Scaled Dot-Product attention\n",
        "            head_v = scaled_dot_product_attention(new_q, new_k, new_v, mask, self.dropout)  # (batch_size, seq_len, d_v)\n",
        "            multi_head_attention_outputs.append(head_v)\n",
        "            \n",
        "        # Concat\n",
        "        #import pdb; pdb.set_trace()\n",
        "        concat = torch.cat(multi_head_attention_outputs, -1)  # (batch_size, seq_len, n_heads*d_v)\n",
        "        \n",
        "        # Linear layer to recover to original shap\n",
        "        output = self.out(concat)  # (batch_size, seq_len, d_model)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djdjfM890LBI"
      },
      "source": [
        "### Feed Forward layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wNVLcVZ70LBI"
      },
      "outputs": [],
      "source": [
        "class FeedForward(torch.nn.Module):\n",
        "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.linear_1 = torch.nn.Linear(d_model, d_ff)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.linear_2 = torch.nn.Linear(d_ff, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.dropout(F.relu(self.linear_1(x)))\n",
        "        x = self.linear_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixWQG_zq0LBJ"
      },
      "source": [
        "### Layer Normalization layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN-S73ls0LBJ"
      },
      "source": [
        "#### Normalization\n",
        "\n",
        "$$\\mu = \\frac{1}{m} \\sum_{i=1}^{m}x_i$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_Mz4_Ry0LBJ"
      },
      "source": [
        "$$\n",
        "\\sigma^{2} = \\frac{1}{m} \\sum^{m}_{i=1}(x_{i} - \\mu)^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvRW7GZ40LBJ"
      },
      "source": [
        "$$\n",
        "\\hat{Z}_i = \\frac{x_i - \\mu_i}{\\sqrt{\\sigma^{2}_{i} + \\epsilon}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibYXTecN0LBK"
      },
      "source": [
        "#### Add two learnable parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPJMJqDP0LBK"
      },
      "source": [
        "$$\n",
        "\\tilde{Z}_i = \\alpha_i * \\hat{Z}_i + \\beta_i\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Z-btuLSY0LBK"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(torch.nn.Module):\n",
        "    def __init__(self, d_model, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.alpha = torch.nn.Parameter(torch.ones(self.d_model))\n",
        "        self.beta = torch.nn.Parameter(torch.zeros(self.d_model))\n",
        "        self.eps = eps\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x size: (batch_size, seq_len, d_model)\n",
        "        x_hat = (x - x.mean(dim=-1, keepdim=True))/(x.std(dim=-1, keepdim=True) + self.eps)\n",
        "        x_tilde = self.alpha*x_hat + self.beta\n",
        "        return x_tilde"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUc35mM50LBK"
      },
      "source": [
        "## Encoder & Decoder layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-woxLvJj0LBK"
      },
      "source": [
        "### Encoder layer\n",
        "\n",
        "An encoder layer contains a multi-head attention layer and feed forward layer\n",
        "\n",
        "![](https://github.com/PytLab/transformer-from-scratch/blob/master/images/encoder.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wS-a-5kQ0LBK"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm_1 = LayerNorm(d_model)\n",
        "        self.norm_2 = LayerNorm(d_model)\n",
        "        self.multi_head_attention = MultiHeadAttention(n_heads, d_model)\n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
        "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        #import pdb; pdb.set_trace()\n",
        "        x = x + self.dropout_1(self.multi_head_attention(x, x, x, mask))\n",
        "        x = self.norm_1(x)\n",
        "        \n",
        "        x = x + self.dropout_2(self.feed_forward(x))\n",
        "        x = self.norm_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgtU7kNv0LBK"
      },
      "source": [
        "### Decoder layer\n",
        "\n",
        "An decoder layer contains two multi-head attention layers and one feed forward layer\n",
        "\n",
        "![](https://github.com/PytLab/transformer-from-scratch/blob/master/images/decoder.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pqHw7jM_0LBL"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(torch.nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm_1 = LayerNorm(d_model)\n",
        "        self.norm_2 = LayerNorm(d_model)\n",
        "        self.norm_3 = LayerNorm(d_model)\n",
        "        \n",
        "        self.dropout_1 = torch.nn.Dropout(dropout)\n",
        "        self.dropout_2 = torch.nn.Dropout(dropout)\n",
        "        self.dropout_3 = torch.nn.Dropout(dropout)\n",
        "        \n",
        "        self.multi_head_attention_1 = MultiHeadAttention(n_heads, d_model)\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(n_heads, d_model)\n",
        "        \n",
        "        self.feed_forward = FeedForward(d_model)\n",
        "        \n",
        "    def forward(self, x, encoder_output, src_mask, trg_mask):\n",
        "        x = self.dropout_1(self.multi_head_attention_1(x, x, x, trg_mask))\n",
        "        x = x + self.norm_1(x)\n",
        "        \n",
        "        x = self.dropout_2(self.multi_head_attention_2(x, encoder_output, encoder_output, src_mask))\n",
        "        x = x + self.norm_2(x)\n",
        "        \n",
        "        x = self.dropout_3(self.feed_forward(x))\n",
        "        x = x + self.norm_3(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hlaDaiRk0LBL"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "\n",
        "def clone_layer(module, N):\n",
        "    return torch.nn.ModuleList([copy.deepcopy(module) for i in range(N)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yh7nvCs0LBL"
      },
      "source": [
        "## Encoder & Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EukdRo420LBL"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
        "        super().__init__()\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.encoder_layers = clone_layer(EncoderLayer(d_model, n_heads), N)\n",
        "        self.norm = LayerNorm(d_model)\n",
        "        \n",
        "    def forward(self, src, mask):\n",
        "        x = self.embed(src)\n",
        "        x = self.pe(x)\n",
        "        for encoder in self.encoder_layers:\n",
        "            x = encoder(x, mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5fsr3G700LBL"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, N, n_heads):\n",
        "        super().__init__()\n",
        "        self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model)\n",
        "        self.decoder_layers = clone_layer(DecoderLayer(d_model, n_heads), N)\n",
        "        self.norm = LayerNorm(d_model)\n",
        "        \n",
        "    def forward(self, trg, encoder_output, src_mask, trg_mask):\n",
        "        x = self.embed(trg)\n",
        "        x = self.pe(x)\n",
        "        for decoder in self.decoder_layers:\n",
        "            x = decoder(x, encoder_output, src_mask, trg_mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "menVDSZf0LBL"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lqjgewBw0LBL"
      },
      "outputs": [],
      "source": [
        "class Transformer(torch.nn.Module):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, N, n_heads):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, d_model, N, n_heads)\n",
        "        self.decoder = Decoder(trg_vocab_size, d_model, N, n_heads)\n",
        "        self.linear = torch.nn.Linear(d_model, trg_vocab_size)\n",
        "        \n",
        "    def forward(self, src, trg, src_mask, trg_mask):\n",
        "        encoder_output = self.encoder(src, src_mask)\n",
        "        decoder_output = self.decoder(trg, encoder_output, src_mask, trg_mask)\n",
        "        output = self.linear(decoder_output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n2Ipxv30LBM"
      },
      "source": [
        "# Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "H2ckIjYc0LBM",
        "outputId": "fae832d0-4626-462b-d4f9-b56a853b28a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.10.0 in /usr/local/lib/python3.7/dist-packages (0.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.1)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "!pip install -U torchtext==0.10.0\n",
        "from torchtext.legacy import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "AkKEN6AR0LBM",
        "outputId": "8fb26752-dd8b-4941-dc80-bcb47c97a244",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Allo', 'la', 'terre']\n"
          ]
        }
      ],
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "tk = nlp.tokenizer(\"I love you so-much!!!\")\n",
        "\n",
        "tokenizer = lambda sentence: [tok.text for tok in nlp.tokenizer(sentence) if tok.text != \" \"]\n",
        "\n",
        "print(tokenizer(\"Allo la terre\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "eiuUBE8v0LBM"
      },
      "outputs": [],
      "source": [
        "SRC = data.Field(lower=True, tokenize=tokenizer)\n",
        "TRG = data.Field(lower=True, tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "icnHdRnT0LBM"
      },
      "outputs": [],
      "source": [
        "src_data = open('data/english.txt', 'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Oi2hQQVZ0LBN"
      },
      "outputs": [],
      "source": [
        "trg_data = open('data/french.txt', 'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "a8vl2tEv0LBN",
        "outputId": "5eddde2b-3540-4c41-fbce-a4fb7e57d7be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-6bc5792262b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'src'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msrc_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trg'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrg_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "raw_data = {'src': [line for line in src_data], 'trg': [line for line in trg_data]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "HiyqQXgi0LBN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "M-Z-8t6E0LBN"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(raw_data, columns=['src', 'trg'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "scrolled": true,
        "id": "viSVf81N0LBN",
        "outputId": "7ea7093a-a13d-44af-d87b-fccaaa459b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                      src  \\\n",
              "0                                                   Go.\\n   \n",
              "1                                                  Run!\\n   \n",
              "2                                                  Run!\\n   \n",
              "3                                                 Fire!\\n   \n",
              "4                                                 Help!\\n   \n",
              "...                                                   ...   \n",
              "154878  \"Top-down economics never works,\" said Obama. ...   \n",
              "154879  A carbon footprint is the amount of carbon dio...   \n",
              "154880  Death is something that we're often discourage...   \n",
              "154881  Since there are usually multiple websites on a...   \n",
              "154882  If someone who doesn't know your background sa...   \n",
              "\n",
              "                                                      trg  \n",
              "0                                                  Va !\\n  \n",
              "1                                               Cours !\\n  \n",
              "2                                              Courez !\\n  \n",
              "3                                              Au feu !\\n  \n",
              "4                                            À l'aide !\\n  \n",
              "...                                                   ...  \n",
              "154878  « L'économie en partant du haut vers le bas, ç...  \n",
              "154879  Une empreinte carbone est la somme de pollutio...  \n",
              "154880  La mort est une chose qu'on nous décourage sou...  \n",
              "154881  Puisqu'il y a de multiples sites web sur chaqu...  \n",
              "154882  Si quelqu'un qui ne connaît pas vos antécédent...  \n",
              "\n",
              "[154883 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d08a5032-fbab-41e7-9f3b-1155be67bd4a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>src</th>\n",
              "      <th>trg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go.\\n</td>\n",
              "      <td>Va !\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Run!\\n</td>\n",
              "      <td>Cours !\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Run!\\n</td>\n",
              "      <td>Courez !\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fire!\\n</td>\n",
              "      <td>Au feu !\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Help!\\n</td>\n",
              "      <td>À l'aide !\\n</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154878</th>\n",
              "      <td>\"Top-down economics never works,\" said Obama. ...</td>\n",
              "      <td>« L'économie en partant du haut vers le bas, ç...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154879</th>\n",
              "      <td>A carbon footprint is the amount of carbon dio...</td>\n",
              "      <td>Une empreinte carbone est la somme de pollutio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154880</th>\n",
              "      <td>Death is something that we're often discourage...</td>\n",
              "      <td>La mort est une chose qu'on nous décourage sou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154881</th>\n",
              "      <td>Since there are usually multiple websites on a...</td>\n",
              "      <td>Puisqu'il y a de multiples sites web sur chaqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154882</th>\n",
              "      <td>If someone who doesn't know your background sa...</td>\n",
              "      <td>Si quelqu'un qui ne connaît pas vos antécédent...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>154883 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d08a5032-fbab-41e7-9f3b-1155be67bd4a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d08a5032-fbab-41e7-9f3b-1155be67bd4a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d08a5032-fbab-41e7-9f3b-1155be67bd4a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "9qg74CpP0LBO"
      },
      "outputs": [],
      "source": [
        "df.to_csv('en_to_fr.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "l7QXOZf80LBO",
        "outputId": "4316db2c-2cda-4ead-af73-a300a06cfe00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  en_to_fr.csv  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "W0jDSvMr0LBO"
      },
      "outputs": [],
      "source": [
        "data_fields = [('src', SRC), ('trg', TRG)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "8aUSLK9n0LBO"
      },
      "outputs": [],
      "source": [
        "train_set = data.TabularDataset('./en_to_fr.csv', format='csv', fields=data_fields)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_set)"
      ],
      "metadata": {
        "id": "kVH6nvS7BD5j",
        "outputId": "b8b52981-e7c6-4a24-9918-94bc4dce01b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torchtext.legacy.data.dataset.TabularDataset object at 0x7f137b2ead90>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wWc_eMf40LBO"
      },
      "outputs": [],
      "source": [
        "SRC.build_vocab(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ZrDEXIXO0LBO",
        "outputId": "8061f174-10af-4de9-c4a7-05c017121f9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13756"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "len(SRC.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "O8BloTMb0LBO"
      },
      "outputs": [],
      "source": [
        "TRG.build_vocab(train_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jEKKrPrD0LBO",
        "outputId": "cecab31b-8ffe-4223-89f9-0440b0dfd5c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26341"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "len(TRG.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "M2DyTxy_0LBP",
        "outputId": "efc6681a-a92b-4a1a-9017-48f47066446a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Dataset.__getattr__ at 0x7f136bec2d50>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "train_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aC95XcmB0LBP"
      },
      "source": [
        "# Train transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRE2pDag0LBP"
      },
      "outputs": [],
      "source": [
        "# set some parameters\n",
        "d_model = 512\n",
        "n_heads = 8\n",
        "N = 6\n",
        "src_vocab_size = len(SRC.vocab)\n",
        "trg_vocab_size = len(TRG.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS1lyjv10LBP"
      },
      "outputs": [],
      "source": [
        "model = Transformer(src_vocab_size, trg_vocab_size, d_model, N, n_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F04yyekX0LBP",
        "outputId": "664cf9d9-8724-44ca-e409-4fb287b5550c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zjshao/.pyenv/versions/3.5.3/envs/ml/lib/python3.5/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "for p in model.parameters():\n",
        "    if p.dim() > 1:\n",
        "        torch.nn.init.xavier_uniform(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYaXCo0O0LBP"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJcmELY80LBP"
      },
      "outputs": [],
      "source": [
        "train_iter = data.Iterator(train_set, batch_size=32, sort_key=lambda x: (len(x.src), len(x.trg)), shuffle=True, train=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrkQnNHd0LBP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_mask(src_input, trg_input):\n",
        "    # Source input mask\n",
        "    pad = SRC.vocab.stoi['<pad>']\n",
        "    src_mask = (src_input != pad).unsqueeze(1)\n",
        "    \n",
        "    # Target input mask\n",
        "    trg_mask = (trg_input != pad).unsqueeze(1)\n",
        "    \n",
        "    seq_len = trg_input.size(1)\n",
        "    nopeak_mask = np.tril(np.ones((1, seq_len, seq_len)), k=0).astype('uint8')\n",
        "    nopeak_mask = torch.from_numpy(nopeak_mask) != 0\n",
        "    trg_mask = trg_mask & nopeak_mask\n",
        "    \n",
        "    return src_mask, trg_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DL9JSouv0LBP"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train_model(n_epochs, output_interval=100):\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        total_loss = 0\n",
        "        for i, batch in enumerate(train_iter):\n",
        "            \n",
        "            src_input = batch.src.transpose(0, 1)  # size (batch_size, seq_len)\n",
        "            trg = batch.trg.transpose(0, 1)  # size (batch_size, seq_len)\n",
        "            \n",
        "            trg_input = trg[:, :-1]\n",
        "            ys = trg[:, 1:].contiguous().view(-1)\n",
        "            \n",
        "            # create src & trg masks\n",
        "            src_mask, trg_mask = create_mask(src_input, trg_input)\n",
        "            preds = model(src_input, trg_input, src_mask, trg_mask)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            loss = F.cross_entropy(preds.view(-1, preds.size(-1)), ys, ignore_index=1)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            total_loss += loss.data[0]\n",
        "\n",
        "            if (i + 1) % output_interval == 0:\n",
        "                avg_loss = total_loss/output_interval\n",
        "                print('time = {}, epoch = {}, iter = {}, loss = {}'.format((time.time() - start)/60,\n",
        "                                                                           epoch + 1,\n",
        "                                                                           i + 1,\n",
        "                                                                           avg_loss))\n",
        "                total_loss = 0\n",
        "                start = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "WOKAA7tz0LBQ",
        "outputId": "21b1f1e6-b235-4b37-e26d-d7ef978ba185"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zjshao/.pyenv/versions/3.5.3/envs/ml/lib/python3.5/site-packages/ipykernel_launcher.py:27: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time = 0.07368873357772827, epoch = 1, iter = 1, loss = 10.275301933288574\n",
            "time = 0.05243396759033203, epoch = 1, iter = 2, loss = 10.036077499389648\n",
            "time = 0.05758536656697591, epoch = 1, iter = 3, loss = 9.876063346862793\n",
            "time = 0.059804431597391766, epoch = 1, iter = 4, loss = 9.840919494628906\n",
            "time = 0.06737850109736125, epoch = 1, iter = 5, loss = 9.733710289001465\n",
            "time = 0.060716116428375246, epoch = 1, iter = 6, loss = 9.621753692626953\n",
            "time = 0.06828871568044027, epoch = 1, iter = 7, loss = 9.605151176452637\n",
            "time = 0.06440561612447103, epoch = 1, iter = 8, loss = 9.512434959411621\n",
            "time = 0.06431623299916585, epoch = 1, iter = 9, loss = 9.478424072265625\n",
            "time = 0.06724570194880168, epoch = 1, iter = 10, loss = 9.49864387512207\n",
            "time = 0.058811533451080325, epoch = 1, iter = 11, loss = 9.33305835723877\n",
            "time = 0.061342501640319826, epoch = 1, iter = 12, loss = 9.282251358032227\n",
            "time = 0.07716486851374309, epoch = 1, iter = 13, loss = 9.268787384033203\n",
            "time = 0.06068573395411173, epoch = 1, iter = 14, loss = 9.16775131225586\n",
            "time = 0.0706778327624003, epoch = 1, iter = 15, loss = 9.151176452636719\n",
            "time = 0.07176573276519775, epoch = 1, iter = 16, loss = 9.124017715454102\n",
            "time = 0.07577134768168131, epoch = 1, iter = 17, loss = 9.080439567565918\n",
            "time = 0.07278071641921997, epoch = 1, iter = 18, loss = 9.008810997009277\n",
            "time = 0.05596356789271037, epoch = 1, iter = 19, loss = 8.948866844177246\n",
            "time = 0.06370005210240683, epoch = 1, iter = 20, loss = 8.894721984863281\n",
            "time = 0.0560766339302063, epoch = 1, iter = 21, loss = 8.81960678100586\n",
            "time = 0.11069205204645792, epoch = 1, iter = 22, loss = 8.837837219238281\n",
            "time = 0.07233175039291381, epoch = 1, iter = 23, loss = 8.66609001159668\n",
            "time = 0.05656316677729289, epoch = 1, iter = 24, loss = 8.57186222076416\n",
            "time = 0.0599276860555013, epoch = 1, iter = 25, loss = 8.567977905273438\n",
            "time = 0.09096531470616659, epoch = 1, iter = 26, loss = 8.546257019042969\n",
            "time = 0.07039418220520019, epoch = 1, iter = 27, loss = 8.51272201538086\n",
            "time = 0.08430409828821818, epoch = 1, iter = 28, loss = 8.435883522033691\n",
            "time = 0.09260969956715902, epoch = 1, iter = 29, loss = 8.334375381469727\n",
            "time = 0.06071858406066895, epoch = 1, iter = 30, loss = 8.334733009338379\n",
            "time = 0.064847465356191, epoch = 1, iter = 31, loss = 8.259394645690918\n",
            "time = 0.10941791534423828, epoch = 1, iter = 32, loss = 8.203652381896973\n",
            "time = 0.07142060200373332, epoch = 1, iter = 33, loss = 8.162467002868652\n",
            "time = 0.06419503291447957, epoch = 1, iter = 34, loss = 8.033669471740723\n",
            "time = 0.09461748202641805, epoch = 1, iter = 35, loss = 8.166277885437012\n",
            "time = 0.07295833031336467, epoch = 1, iter = 36, loss = 8.03286361694336\n",
            "time = 0.08415588537851969, epoch = 1, iter = 37, loss = 7.826048374176025\n",
            "time = 0.08624071677525838, epoch = 1, iter = 38, loss = 7.89840841293335\n",
            "time = 0.07309126456578573, epoch = 1, iter = 39, loss = 7.866915702819824\n",
            "time = 0.06748514970143636, epoch = 1, iter = 40, loss = 7.68775749206543\n",
            "time = 0.0692827820777893, epoch = 1, iter = 41, loss = 7.623300552368164\n",
            "time = 0.06189734935760498, epoch = 1, iter = 42, loss = 7.6679487228393555\n",
            "time = 0.08047765096028646, epoch = 1, iter = 43, loss = 7.521366119384766\n",
            "time = 0.0684423009554545, epoch = 1, iter = 44, loss = 7.446471691131592\n",
            "time = 0.07436726490656535, epoch = 1, iter = 45, loss = 7.616914749145508\n",
            "time = 0.06415958007176717, epoch = 1, iter = 46, loss = 7.335622310638428\n",
            "time = 0.057961066563924156, epoch = 1, iter = 47, loss = 7.369991302490234\n",
            "time = 0.06256914933522542, epoch = 1, iter = 48, loss = 7.360755443572998\n",
            "time = 0.07985510031382242, epoch = 1, iter = 49, loss = 7.160858154296875\n",
            "time = 0.08559258381525675, epoch = 1, iter = 50, loss = 7.240228652954102\n",
            "time = 0.08142703374226888, epoch = 1, iter = 51, loss = 7.339879989624023\n",
            "time = 0.0808611512184143, epoch = 1, iter = 52, loss = 7.184959411621094\n",
            "time = 0.06521214644114176, epoch = 1, iter = 53, loss = 7.111505031585693\n",
            "time = 0.07443951765696208, epoch = 1, iter = 54, loss = 7.1437859535217285\n",
            "time = 0.061329265435536705, epoch = 1, iter = 55, loss = 6.9383697509765625\n",
            "time = 0.06178390185038249, epoch = 1, iter = 56, loss = 7.023250102996826\n",
            "time = 0.07523408333460489, epoch = 1, iter = 57, loss = 6.823822021484375\n",
            "time = 0.07574890057245891, epoch = 1, iter = 58, loss = 7.063283443450928\n",
            "time = 0.06116951704025268, epoch = 1, iter = 59, loss = 6.806905746459961\n",
            "time = 0.04993902047475179, epoch = 1, iter = 60, loss = 6.84621524810791\n",
            "time = 0.06731653610865275, epoch = 1, iter = 61, loss = 6.785461902618408\n",
            "time = 0.05475561618804932, epoch = 1, iter = 62, loss = 6.7967143058776855\n",
            "time = 0.06382323106129964, epoch = 1, iter = 63, loss = 6.607127666473389\n",
            "time = 0.061555349826812746, epoch = 1, iter = 64, loss = 6.847414493560791\n",
            "time = 0.07733279863993327, epoch = 1, iter = 65, loss = 6.746364593505859\n",
            "time = 0.0634850025177002, epoch = 1, iter = 66, loss = 6.606636047363281\n",
            "time = 0.07392118374506633, epoch = 1, iter = 67, loss = 6.816209316253662\n",
            "time = 0.06916069984436035, epoch = 1, iter = 68, loss = 6.543198585510254\n",
            "time = 0.06224756638209025, epoch = 1, iter = 69, loss = 6.31641149520874\n",
            "time = 0.06160989999771118, epoch = 1, iter = 70, loss = 6.547100067138672\n",
            "time = 0.06779568592707316, epoch = 1, iter = 71, loss = 6.4370036125183105\n",
            "time = 0.06392256418863933, epoch = 1, iter = 72, loss = 6.589992046356201\n",
            "time = 0.08541620175043742, epoch = 1, iter = 73, loss = 6.499094009399414\n",
            "time = 0.09481403430302938, epoch = 1, iter = 74, loss = 6.3581223487854\n",
            "time = 0.059490633010864255, epoch = 1, iter = 75, loss = 6.293521404266357\n",
            "time = 0.07245870033899943, epoch = 1, iter = 76, loss = 6.472769260406494\n",
            "time = 0.05217263698577881, epoch = 1, iter = 77, loss = 6.150349140167236\n",
            "time = 0.10459943215052286, epoch = 1, iter = 78, loss = 6.43544340133667\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-09835295c489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-38-f9d81b73299c>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(n_epochs, output_interval)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.5.3/envs/ml/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.pyenv/versions/3.5.3/envs/ml/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_model(3, output_interval=1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}